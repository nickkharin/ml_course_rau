# -*- coding: utf-8 -*-
"""ML_exam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dcbmbse0dODQLXNvvyCWJuztaiQ1FIQf

The deadline for this homework is on **15.01.2024** (right before the practice session). After completing the exercises, you should

1. Download this file into your computer (`File` $\to$ `Download .ipynb`)

2. Download the python files (`.py`) (*if there are any attached to the homework that you need to complete*).

3. Compress the above files (`.zip` `.rar`) and name the compressed file in the following way *Final_NameSurname* (for example `Final_NshanPotikyan.zip`)

4. Send the compressed file to this email address `nshan.potikyan@gmail.com` with subject **Final**

# Kaggle Competition on Mashtots Dataset

In this homework, you do not need to implement any algorithm from scratch, instead you will try to create an algorithm in order to classify Armenian handwritten characters. Do the following steps in order to get started:

* Visit [the page of the competition]https://www.kaggle.com/c/mashtots-dataset-v2/overview) and press "Join Competition"

* You will need to sign in to kaggle if you have an account already and if you don't, you will need to create one

* After signing in you will need to accept the rules of the competition by pressing "I understand and accept"

* Go to the "Data" tab, read about the dataset and then download the dataset files

* Upload the dataset into Google Drive, so that you can access it from Colab

* At this point you should be able to start your experiments.

# Mounting Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

cd drive/MyDrive/ML/MashtotsDataset

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c mashtots-dataset-v2

!unzip mashtots-dataset-v2.zip

import pandas as pd
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid') # Plot style
plt.rcParams['figure.figsize'] = (12.0, 8.0)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

"""## Data Processing

If you have noticed, the training data consists of raw images, which means that we need to process those images and construct a dataset (pd.DataFrame or np.array) where each row will be an image vector and the last column will be its corresponding label. Complete the missing parts of the code snippet below in order to construct the dataset.
"""

root_dir = "Train/Train"

# getting the list of folder names (label encodings)
sub_folders = os.listdir(root_dir)

# empty list for the data
train = []

# going over the folders
count = 0
total_images = sum([len(files) for r, d, files in os.walk(root_dir)])

for sub_folder in sub_folders:
    images = os.listdir(os.path.join(root_dir, sub_folder))
    for image_name in images:
        image_path = os.path.join(root_dir, sub_folder, image_name)
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        if image is not None:
            if image.shape[0] != 64 or image.shape[1] != 64:
                image = cv2.resize(image, (64, 64))

            feature_vector = image.flatten()
            train.append([*feature_vector, int(sub_folder)])
        else:
            print(f"Не удалось загрузить изображение: {image_path}")

        count += 1
        if count % 100 == 0:
            print(f"Обработано изображений: {count} из {total_images}")

print(f"Всего обработано изображений: {count}")

train = pd.DataFrame(train)

"""Have a look the resulting data data"""

train.head()

"""It's a good idea to save the processed dataset in your Drive, so that you don't need go through the above process every time."""

train.to_csv('train.csv', index=False)

train = pd.read_csv('/content/drive/MyDrive/ML/MashtotsDataset/train.csv')

feature_columns = [f'pixel_{i}' for i in range(4096)]  # 4096 = 64x64
train.columns = feature_columns + ['label']

"""## Constructing a Validation set

Since you do not have a separate labeled test data for validation, you should construct (put aside) your own validation set from the whole training data, in order to choose good models.
You are free to decide how much data you will use for what purposes, but remember to use data wisely.
"""

from sklearn.model_selection import train_test_split

X = train[feature_columns]
y = train['label']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train = X_train.reshape(-1, 64, 64, 1)
X_val = X_val.reshape(-1, 64, 64, 1)

X_train = X_train / 255.0
X_val = X_val / 255.0

"""## Modelling

Perform experiments with different algorithms that we have learned and choose a good algorithm for final evaluation.
"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

model = Sequential()

model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(64, 64, 1)))
model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(len(sub_folders), activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization
from keras.regularizers import l2

model = Sequential()

model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(64, 64, 1)))
model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.25))

model.add(GlobalAveragePooling2D())

model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(len(sub_folders), activation='softmax', kernel_regularizer=l2(0.001)))  # len(sub_folders) - количество классов

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))

test_loss, test_accuracy = model.evaluate(X_val, y_val)
print('Точность на валидационных данных:', test_accuracy)

"""## Generating predictions

At this stage you should have good performing model that you want to test on kaggle.
"""

test = pd.read_csv('/content/drive/MyDrive/ML/MashtotsDataset/new_test/new_test.csv')
test.head()

X_test = test.values

X_test = X_test.reshape(-1, 64, 64, 1)

X_test = X_test / 255.0

predictions = model.predict(X_test)
test_loss, test_accuracy = model.evaluate(X_val, y_val)
predicted_classes = np.argmax(predictions, axis=1)
submission_df = pd.DataFrame({'Id': np.arange(1, X_test.shape[0]+1),
                              'Category': predicted_classes})
submission_df.to_csv('/content/drive/MyDrive/ML/MashtotsDataset/my_submission6.csv', index=False)

"""* Download the generated ``my_submission.csv`` file and upload it to kaggle by pressing "Submit predictions" and following the steps.

* You have to describe your submission under the "Describe Submission" section with some details about the algorithm/hyperparameters that you used

* Take a screenshot of the kaggle web page after each submission, where one can see the submission file name, its description and the achieved accuracy score.

* **You will need to send those screenshots along with this notebook**

* After the submission you will see the accuracy of your model and your position in the public leaderboard

* You can only submit 5 files per day

* You can not participate in teams (for this homework)

# Motivation

* You need to make at least 5 submissions with accuracy **above 40%** in order to get full grade from this homework

* Those of you who get over 40% will get the corresponding grade

* The person who gets the highest score among all participants will get 100%.
"""